# Performance Results

The last complete run of this benchmark setup yielded the results presented
below. This report was generated on `r Sys.time()`.

```{r load-scripts, echo=FALSE, include=FALSE}
# load libraries, the data, and prepare it
if (Sys.getenv("RSTUDIO") == "1") { setwd("/Users/smarr/Projects/PostDoc/FASTXX/are-we-fast-yet/report") }
source("scripts/libraries.R", chdir=TRUE)
opts_chunk$set(dev = 'png',
               dev.args=list(pointsize=10),
               echo = FALSE,
               fig.keep='all',
               fig.path="figures/",
               external=FALSE,
               tidy=FALSE)
#    cache=TRUE,

vm_names <- c(
  "Crystal"               = "Crystal",
  "GraalBasic"            = "Graal Core",
  "GraalC2"               = "HotSpot C2, jvmi",
  "GraalEnterprise"       = "Graal VM",
  "GraalJS"               = "Graal.js",
  
  "JRubyC2"               = "JRuby (C2, jvmci)",
  "JRubyGraal"            = "JRuby (Graal Core)",
  "JRubyJ8"               = "JRuby (C2, Java)",
  
  "Topaz"                 = "Topaz",
  
  "JRubyTruffle"           = "TruffleRuby (core)",
  "JRubyTruffleEnterprise" = "TruffleRuby (GraalVM)",
  
  "Java8U66"              = "HotSpot C2",
  "JavaInt"               = "HotSpot Int",
  
  "Lua53"                 = "Lua 5.3",
  "LuaJIT2"               = "LuaJIT 2",
  "MRI23"                 = "Ruby MRI 2.3",
  "Node"                  = "Node.js",
  "NodeTurboFan"          = "Node.js, TurboFan",
  "Pharo"                 = "Pharo",
  "RBX314"                = "Rubinius",
  "SOMns"                 = "SOMns core",
  "SOMns-Enterprise"      = "SOMns GraalVM",
  "SOMnsInt"              = "SOMns int",
  "TruffleSOM"            = "TruffleSOM core",
  "TruffleSOM-Enterprise" = "TruffleSOM GraalVM",
  "TruffleSOM-TOM"        = "TruffleSOM core, Truffle OM",
  "TruffleSOM-TOM-Enterprise" = "TruffleSOM GraalVM, Truffle OM")

vms_all  <- names(vm_names)
vms_slow <- c("JRubyC2", "JRubyGraal", "JRubyJ8",
              "JavaInt",
              "Lua53", "LuaJIT2",
              "MRI23", "RBX314",
              "Pharo", "SOMnsInt", "Topaz")
vms_fast <- c("Crystal",
              "GraalBasic", "GraalC2", "GraalEnterprise",
              "GraalJS",
              "JRubyTruffle", "JRubyTruffleEnterprise",
              "Java8U66",
              "Node", "NodeTurboFan",
              "SOMns", "SOMns-Enterprise",
              "TruffleSOM", "TruffleSOM-TOM", "TruffleSOM-Enterprise", "TruffleSOM-TOM-Enterprise")
vms_truffle <- c("GraalJS",
                 "JRubyTruffle", "JRubyTruffleEnterprise",
                 "TruffleSOM", "TruffleSOM-Enterprise", "TruffleSOM-TOM", "TruffleSOM-TOM-Enterprise",
                 "SOMns", "SOMns-Enterprise")

assert_that(all(sort(c(vms_slow, vms_fast)) == sort(vms_all))) ## sanity check

# vm_colors <- brewer.pal(length(vms_all), "Paired")  # to replace scale_fill_brewer(type = "qual", palette = "Paired")
vm_colors <- rainbow(length(vms_all))

names(vm_colors) <- vm_names

data <- load_all_data("data/all", "benchmark.data")
data <- droplevels(subset(data,
                          select = c(Value, Unit, Benchmark, VM, Suite, Extra, Iteration, version, sha)))

data_fast_vms      <- droplevels(subset(data, Iteration >= 500 & Iteration <= 1000 & VM %in% vms_fast))
data_very_slow_vms <- droplevels(subset(data, VM %in% vms_slow & VM != "JRubyJ8"))
data_slow_vms      <- droplevels(subset(data, Iteration >= 100 & (VM == "JRubyJ8" | VM == "Pharo")))
data <- rbind(data_fast_vms, data_slow_vms, data_very_slow_vms)

#norm <- ddply(data, ~ Benchmark, transform,
#              RuntimeRatio = Value / mean(Value[VM == "Java8U66"]))

norm <- transform(data, RuntimeRatio = Value / mean(Value[VM == "Java8U66"]))
stats <- norm %>%
  group_by(VM, Benchmark, Suite, version, sha) %>%
  summarise(
    Time.ms = mean(Value),
    sd      = sd(Value),
    RuntimeFactor = mean(RuntimeRatio),
    RR.sd         = sd(RuntimeRatio),
    RR.median     = median(RuntimeRatio))

stats_vm <- stats %>%
  group_by(VM, Suite, version, sha) %>%
  mutate(
    VMMean = geometric.mean(RuntimeFactor),
    min = min(RuntimeFactor),
    max = max(RuntimeFactor))

stats_latest <- stats_vm %>%
  group_by(VM) %>%
  filter(version == max(version))

plot_benchmarks_speedup_for_vms <- function(stats, vms) {
  vm_stats <- droplevels(subset(stats, VM %in% vms))

  for (b in levels(vm_stats$Benchmark)) {
    data_b <- droplevels(subset(vm_stats, Benchmark == b))

    p <- ggplot(data_b, aes(x = VM, y = RuntimeFactor, fill = VM)) +
      geom_bar(stat = "identity") +
      geom_errorbar(aes(ymax = RuntimeFactor + RR.sd, ymin = RuntimeFactor - RR.sd), width=0.25) +
      coord_flip() + theme_bw() + # scale_fill_manual(values=col) +
      theme(legend.position="none") + ggtitle(b)
    tryCatch({print(p)})
  }
}

plot_benchmarks_speedup_for_vms_faceted <- function(
  stats, vms, ylab = "Runtime Factor, normalized to Java\n(lower is better)") {
  vm_stats <- subset(stats, VM %in% vms)
  vm_stats$VM <- revalue(vm_stats$VM, vm_names)
  vm_stats$VM <- reorder(vm_stats$VM, X=vm_stats$VMMean)
  breaks <- levels(droplevels(vm_stats)$VM)
  col_values <- sapply(breaks, function(x) vm_colors[[x]])

  p <- ggplot(vm_stats, aes(x = VM, y = RuntimeFactor, fill = VM)) +
      geom_bar(stat = "identity") +
      geom_errorbar(aes(ymax = RuntimeFactor + RR.sd, ymin = RuntimeFactor - RR.sd), width=0.25) +
      facet_wrap(~ Benchmark, ncol = 1, scales="free_y") +
       theme_bw() + theme_simple(font_size = 8) + # scale_fill_manual(values=col) + coord_flip() +
      theme(legend.position="none", axis.text.x=element_text(angle=90, hjust = 1, vjust = 0.5)) +
    scale_fill_manual(values = col_values) +
    ylab(ylab)
  print(p)
}

overview_box_plot <- function(stats, vms, prepare_data = NULL, pre_plot = NULL, new_colors = FALSE) {
  # stats <- stats_latest
  # vms <- c("Node", "Pharo", "JavaInt", 
  #          "Lua53", "LuaJIT2", "SOMns-Enterprise")

  vm_stats <- stats %>%
    filter(VM %in% vms)

  vm_stats$VM <- revalue(vm_stats$VM, vm_names)
  vm_stats$VM <- reorder(vm_stats$VM, X=-vm_stats$VMMean)
  if (!is.null(prepare_data)) {
   vm_stats <- prepare_data(vm_stats)
  }
  vm_stats <- droplevels(vm_stats)

  breaks <- levels(vm_stats$VM)
  cat(breaks)
  if (new_colors) {
    col_values <- brewer.pal(length(breaks), "Paired")
  } else {
    col_values <- sapply(breaks, function(x) vm_colors[[x]])
  }

  plot <- ggplot(vm_stats, aes(x=VM, y=RuntimeFactor, fill = VM))
  if (!is.null(pre_plot)) {
    plot <- pre_plot(plot)
  }
  plot <- plot +
    geom_boxplot(outlier.size = 0.5) + #fill=get_color(5, 7)
    theme_bw() + theme_simple(font_size = 8) +
    theme(axis.text.x = element_text(angle= 90, vjust=0.5, hjust=1), legend.position="none") +
    #scale_y_log10(breaks=c(1,2,3,10,20,30,50,100,200,300,500,1000)) + #limit=c(0,30), breaks=seq(0,100,5), expand = c(0,0)
    ggtitle("Runtime Factor, normalized to Java\n(lower is better)") + coord_flip() + xlab("") +
    scale_fill_manual(values = col_values)
  plot
}
```

All results are normalized to Java 1.8.0_91. Furthermore, we report peak
performance. This means, the reported measurements are taken after warmup and
compilation of the benchmark code is completed.

## Overview

##### Fast Language Implementations

The following set of language implementations reaches the performance of Java on
our set of benchmarks within a factor of 2 to 3 on average. To allow for a more
detailed assessment of these *fast* language implementations, we exclude slower
ones from the following plot.

```{r fast-langs-overview, fig.width=15, fig.height=6}
# overview_box_plot(stats_latest,
#                   c("Node", "Pharo", "JavaInt", "Java8U66", "MRI23", "Lua53", "LuaJIT2", "SOMns-Enterprise"), new_colors = TRUE)
p <- overview_box_plot(stats_latest, vms_fast, pre_plot = function (p) {
  p + geom_hline(aes(yintercept=1), colour="#cccccc", linetype="dashed") +
      geom_hline(aes(yintercept=2), colour="#cccccc", linetype="dashed") +
      geom_hline(aes(yintercept=3), colour="#cccccc", linetype="dashed") })
p + scale_y_continuous(limit=c(0,13), breaks = c(1, 2, 3, 4, 6, 8, 10, 12))
```

##### All Language Implementations

Other language implementations are not necessarily reaching performance similar
to Java on our benchmarks. The following plot include all of the
implementations.

```{r all-langs-overview, fig.width=8, fig.height=10}
p <- overview_box_plot(stats_latest, vms_all)
p + scale_y_continuous(breaks = c(0, 5, seq(from=10, by=10, to=100)))
```

###### Ruby Performance

```{r ruby-overview, fig.width=8, fig.height=6}
p <- overview_box_plot(stats_latest, c("Crystal", "Java8U66", "RBX314", "Topaz", "MRU23", "JRubyC2", "JRubyTruffleEnterprise", "Pharo"))
p + scale_y_continuous(breaks = c(0, 5, seq(from=10, by=10, to=100)))
```

##### Performance Overview Data
<a id="data-table"></a>

The following table contains the numerical representation of the results
depicted above.

```{r truffle-lang-table, results='asis', echo=FALSE}
vm_stats <- ddply(stats_latest, ~ VM, summarise,
                     geomean = geometric.mean(RuntimeFactor),
                     sd      = sd(RuntimeFactor),
                     min     = min(RuntimeFactor),
                     max     = max(RuntimeFactor),
                     median  = median(RuntimeFactor))
vm_stats$VM <- revalue(vm_stats$VM, vm_names)
vm_stats$VM <- reorder(vm_stats$VM, X=vm_stats$geomean)


t <- tabular(Justify("l")*Heading()*VM ~
             Heading('Runtime Factor over Java')*Justify("r")*Format(sprintf("%.2f"))*((geomean + sd + min + max + median)*Heading()*identity), data=vm_stats)
table_options(justification="c ")
html(t)
```

## Details for all Benchmarks
<a id="all-benchmarks"></a>

The following plots show results for each of the benchmarks.

##### Fast Language Implementations

```{r fast-langs-benchmarks, fig.width=4, fig.height=16}
plot_benchmarks_speedup_for_vms_faceted(stats_latest, vms_fast)
```

##### Slow Language Implementations

```{r slow-langs-benchmarks, fig.width=4, fig.height=16}
plot_benchmarks_speedup_for_vms_faceted(stats_latest, vms_slow)
```

##### Benchmark Results
<a id="benchmark-table"></a>

The following table contains the numerical results for all benchmarks.

```{r benchmark-table, results='asis', echo=FALSE}
t_stats <- stats_latest
t_stats$VM <- revalue(t_stats$VM, vm_names)
t_stats$VM <- reorder(t_stats$VM, X=t_stats$VMMean)

show_plain <- mean ## this is silly, but works better than the identity for missing values

t <- tabular(Justify("l")*Heading()*Benchmark*VM ~
             Heading('Runtime Factor over Java')*Justify("r")*Format(sprintf("%.2f"))*((
                 Heading("mean")*RuntimeFactor
               + Heading("sd")*RR.sd
               # + Heading("median")*RR.median
               )*Heading()*show_plain), data=t_stats)
html(t)
```
